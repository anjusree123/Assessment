{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM+6XZYAHAtBx2TUlk+qH9k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjusree123/TensorFlow/blob/main/Video_Vision_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPzXRXlUlMrh",
        "outputId": "52abb0ca-bbb5-470a-d489-24232f832c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq medmnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading libraries"
      ],
      "metadata": {
        "id": "uTfctOueqDWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import imageio\n",
        "import medmnist\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "163Fk01NqG9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "rclWgB7It-et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "DATASET_NAME = \"organmnist3d\"\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (28, 28, 28, 1)\n",
        "NUM_CLASSES = 11\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 60\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = (8, 8, 8)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8"
      ],
      "metadata": {
        "id": "o1ObuE5OqPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "AQXASMg1ueCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_prepare_dataset(data_info: dict):\n",
        "    \"\"\"Utility function to download the dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_info (dict): Dataset metadata.\n",
        "    \"\"\"\n",
        "    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
        "\n",
        "    with np.load(data_path) as data:\n",
        "        # Get videos\n",
        "        train_videos = data[\"train_images\"]\n",
        "        valid_videos = data[\"val_images\"]\n",
        "        test_videos = data[\"test_images\"]\n",
        "\n",
        "        # Get labels\n",
        "        train_labels = data[\"train_labels\"].flatten()\n",
        "        valid_labels = data[\"val_labels\"].flatten()\n",
        "        test_labels = data[\"test_labels\"].flatten()\n",
        "\n",
        "    return (\n",
        "        (train_videos, train_labels),\n",
        "        (valid_videos, valid_labels),\n",
        "        (test_videos, test_labels),\n",
        "    )\n",
        "\n",
        "\n",
        "# Get the metadata of the dataset\n",
        "info = medmnist.INFO[DATASET_NAME]\n",
        "\n",
        "# Get the dataset\n",
        "prepared_dataset = download_and_prepare_dataset(info)\n",
        "(train_videos, train_labels) = prepared_dataset[0]\n",
        "(valid_videos, valid_labels) = prepared_dataset[1]\n",
        "(test_videos, test_labels) = prepared_dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Deq77FdQugSk",
        "outputId": "afcc7c5e-099f-47fd-8c7c-c7d5403b5b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://zenodo.org/record/6496656/files/organmnist3d.npz?download=1\n",
            "32657407/32657407 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
        "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
        "    # Preprocess images\n",
        "    frames = tf.image.convert_image_dtype(\n",
        "        frames[\n",
        "            ..., tf.newaxis\n",
        "        ],  # The new axis is to help for further processing with Conv3D layers\n",
        "        tf.float32,\n",
        "    )\n",
        "    # Parse label\n",
        "    label = tf.cast(label, tf.float32)\n",
        "    return frames, label\n",
        "\n",
        "\n",
        "def prepare_dataloader(\n",
        "    videos: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    loader_type: str = \"train\",\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "):\n",
        "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
        "\n",
        "    if loader_type == \"train\":\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
        "\n",
        "    dataloader = (\n",
        "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
        "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
        "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
      ],
      "metadata": {
        "id": "xv2y-YXculES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tubelet embedding"
      ],
      "metadata": {
        "id": "uFzB5hh-wIYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.projection = layers.Conv3D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos):\n",
        "        projected_patches = self.projection(videos)\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches"
      ],
      "metadata": {
        "id": "Xlqe6DS6vi58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens"
      ],
      "metadata": {
        "id": "TnY8PksUxVSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    transformer_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    embed_dim=PROJECTION_DIM,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "):\n",
        "    # Get the input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = tubelet_embedder(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = positional_encoder(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization and MHSA\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer Normalization and MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
        "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
        "            ]\n",
        "        )(x3)\n",
        "\n",
        "        # Skip connection\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Layer normalization and Global average pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "    representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # Classify outputs.\n",
        "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "lbE0-QBYxqxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "7DbF5ydFxzCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment():\n",
        "    # Initialize model\n",
        "    model = create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
        "        ),\n",
        "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
        "    )\n",
        "\n",
        "    # Compile the model with the optimizer, loss function\n",
        "    # and the metrics.\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Train the model.\n",
        "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "model = run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VExIAguxw4D",
        "outputId": "935eb98a-2fa2-4a3a-fe01-1db237e9a5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "31/31 [==============================] - 28s 256ms/step - loss: 2.5249 - accuracy: 0.1265 - top-5-accuracy: 0.5782 - val_loss: 2.3921 - val_accuracy: 0.1366 - val_top-5-accuracy: 0.5652\n",
            "Epoch 2/60\n",
            "31/31 [==============================] - 6s 203ms/step - loss: 2.2072 - accuracy: 0.1934 - top-5-accuracy: 0.6842 - val_loss: 2.0765 - val_accuracy: 0.2795 - val_top-5-accuracy: 0.7205\n",
            "Epoch 3/60\n",
            "31/31 [==============================] - 6s 193ms/step - loss: 2.0113 - accuracy: 0.2387 - top-5-accuracy: 0.7963 - val_loss: 1.7653 - val_accuracy: 0.3851 - val_top-5-accuracy: 0.8385\n",
            "Epoch 4/60\n",
            "31/31 [==============================] - 6s 192ms/step - loss: 1.8657 - accuracy: 0.2860 - top-5-accuracy: 0.8333 - val_loss: 1.5736 - val_accuracy: 0.3416 - val_top-5-accuracy: 0.8820\n",
            "Epoch 5/60\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 1.7039 - accuracy: 0.3467 - top-5-accuracy: 0.8776 - val_loss: 1.2881 - val_accuracy: 0.4596 - val_top-5-accuracy: 0.9814\n",
            "Epoch 6/60\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 1.5121 - accuracy: 0.4074 - top-5-accuracy: 0.9167 - val_loss: 1.2419 - val_accuracy: 0.4472 - val_top-5-accuracy: 0.9503\n",
            "Epoch 7/60\n",
            "31/31 [==============================] - 6s 204ms/step - loss: 1.3968 - accuracy: 0.4506 - top-5-accuracy: 0.9342 - val_loss: 1.1226 - val_accuracy: 0.5466 - val_top-5-accuracy: 0.9752\n",
            "Epoch 8/60\n",
            "31/31 [==============================] - 6s 200ms/step - loss: 1.3860 - accuracy: 0.4516 - top-5-accuracy: 0.9352 - val_loss: 1.0230 - val_accuracy: 0.5590 - val_top-5-accuracy: 0.9814\n",
            "Epoch 9/60\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 1.1738 - accuracy: 0.5453 - top-5-accuracy: 0.9516 - val_loss: 1.0528 - val_accuracy: 0.5652 - val_top-5-accuracy: 0.9752\n",
            "Epoch 10/60\n",
            "31/31 [==============================] - 6s 202ms/step - loss: 1.2743 - accuracy: 0.5082 - top-5-accuracy: 0.9465 - val_loss: 0.8888 - val_accuracy: 0.6708 - val_top-5-accuracy: 0.9876\n",
            "Epoch 11/60\n",
            "31/31 [==============================] - 6s 199ms/step - loss: 1.0836 - accuracy: 0.5772 - top-5-accuracy: 0.9496 - val_loss: 0.9466 - val_accuracy: 0.6025 - val_top-5-accuracy: 0.9752\n",
            "Epoch 12/60\n",
            "31/31 [==============================] - 6s 203ms/step - loss: 1.1036 - accuracy: 0.5813 - top-5-accuracy: 0.9599 - val_loss: 0.8107 - val_accuracy: 0.6957 - val_top-5-accuracy: 0.9938\n",
            "Epoch 13/60\n",
            "31/31 [==============================] - 7s 239ms/step - loss: 0.9545 - accuracy: 0.6389 - top-5-accuracy: 0.9640 - val_loss: 0.7699 - val_accuracy: 0.7329 - val_top-5-accuracy: 0.9814\n",
            "Epoch 14/60\n",
            "31/31 [==============================] - 6s 195ms/step - loss: 0.8937 - accuracy: 0.6502 - top-5-accuracy: 0.9815 - val_loss: 0.6760 - val_accuracy: 0.7205 - val_top-5-accuracy: 1.0000\n",
            "Epoch 15/60\n",
            "31/31 [==============================] - 6s 190ms/step - loss: 0.8170 - accuracy: 0.6965 - top-5-accuracy: 0.9835 - val_loss: 0.5133 - val_accuracy: 0.8385 - val_top-5-accuracy: 0.9938\n",
            "Epoch 16/60\n",
            "31/31 [==============================] - 6s 204ms/step - loss: 0.7478 - accuracy: 0.7150 - top-5-accuracy: 0.9877 - val_loss: 0.5219 - val_accuracy: 0.8509 - val_top-5-accuracy: 0.9938\n",
            "Epoch 17/60\n",
            "31/31 [==============================] - 6s 196ms/step - loss: 0.6891 - accuracy: 0.7593 - top-5-accuracy: 0.9856 - val_loss: 0.6772 - val_accuracy: 0.8012 - val_top-5-accuracy: 0.9814\n",
            "Epoch 18/60\n",
            "31/31 [==============================] - 6s 197ms/step - loss: 0.6208 - accuracy: 0.7870 - top-5-accuracy: 0.9907 - val_loss: 0.6027 - val_accuracy: 0.7764 - val_top-5-accuracy: 0.9876\n",
            "Epoch 19/60\n",
            "31/31 [==============================] - 7s 219ms/step - loss: 0.4928 - accuracy: 0.8200 - top-5-accuracy: 0.9959 - val_loss: 0.4136 - val_accuracy: 0.8758 - val_top-5-accuracy: 0.9876\n",
            "Epoch 20/60\n",
            "31/31 [==============================] - 6s 204ms/step - loss: 0.5223 - accuracy: 0.8045 - top-5-accuracy: 0.9918 - val_loss: 0.4938 - val_accuracy: 0.8447 - val_top-5-accuracy: 0.9876\n",
            "Epoch 21/60\n",
            "31/31 [==============================] - 7s 220ms/step - loss: 0.4374 - accuracy: 0.8374 - top-5-accuracy: 0.9969 - val_loss: 0.3887 - val_accuracy: 0.8820 - val_top-5-accuracy: 1.0000\n",
            "Epoch 22/60\n",
            "31/31 [==============================] - 6s 203ms/step - loss: 0.3680 - accuracy: 0.8673 - top-5-accuracy: 0.9990 - val_loss: 0.3915 - val_accuracy: 0.8758 - val_top-5-accuracy: 0.9876\n",
            "Epoch 23/60\n",
            "31/31 [==============================] - 7s 220ms/step - loss: 0.3657 - accuracy: 0.8683 - top-5-accuracy: 0.9979 - val_loss: 0.3851 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9876\n",
            "Epoch 24/60\n",
            "31/31 [==============================] - 9s 278ms/step - loss: 0.3074 - accuracy: 0.8899 - top-5-accuracy: 0.9979 - val_loss: 0.3451 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9938\n",
            "Epoch 25/60\n",
            "31/31 [==============================] - 8s 257ms/step - loss: 0.2482 - accuracy: 0.9136 - top-5-accuracy: 0.9990 - val_loss: 0.3676 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9876\n",
            "Epoch 26/60\n",
            "31/31 [==============================] - 7s 237ms/step - loss: 0.2768 - accuracy: 0.9002 - top-5-accuracy: 1.0000 - val_loss: 0.4354 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9876\n",
            "Epoch 27/60\n",
            "31/31 [==============================] - 7s 236ms/step - loss: 0.2561 - accuracy: 0.9105 - top-5-accuracy: 0.9979 - val_loss: 0.3519 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9938\n",
            "Epoch 28/60\n",
            "31/31 [==============================] - 7s 228ms/step - loss: 0.1616 - accuracy: 0.9414 - top-5-accuracy: 1.0000 - val_loss: 0.3104 - val_accuracy: 0.9317 - val_top-5-accuracy: 1.0000\n",
            "Epoch 29/60\n",
            "31/31 [==============================] - 7s 229ms/step - loss: 0.1802 - accuracy: 0.9434 - top-5-accuracy: 1.0000 - val_loss: 0.3059 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
            "Epoch 30/60\n",
            "31/31 [==============================] - 7s 226ms/step - loss: 0.1200 - accuracy: 0.9619 - top-5-accuracy: 1.0000 - val_loss: 0.3469 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
            "Epoch 31/60\n",
            "31/31 [==============================] - 7s 236ms/step - loss: 0.1141 - accuracy: 0.9660 - top-5-accuracy: 1.0000 - val_loss: 0.3337 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
            "Epoch 32/60\n",
            "31/31 [==============================] - 7s 232ms/step - loss: 0.1031 - accuracy: 0.9671 - top-5-accuracy: 1.0000 - val_loss: 0.5211 - val_accuracy: 0.8634 - val_top-5-accuracy: 0.9814\n",
            "Epoch 33/60\n",
            "31/31 [==============================] - 7s 229ms/step - loss: 0.2211 - accuracy: 0.9218 - top-5-accuracy: 0.9979 - val_loss: 0.5847 - val_accuracy: 0.8199 - val_top-5-accuracy: 1.0000\n",
            "Epoch 34/60\n",
            "31/31 [==============================] - 7s 222ms/step - loss: 0.2239 - accuracy: 0.9259 - top-5-accuracy: 0.9990 - val_loss: 0.3554 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
            "Epoch 35/60\n",
            "31/31 [==============================] - 6s 191ms/step - loss: 0.0881 - accuracy: 0.9753 - top-5-accuracy: 1.0000 - val_loss: 0.3733 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
            "Epoch 36/60\n",
            "31/31 [==============================] - 6s 204ms/step - loss: 0.0884 - accuracy: 0.9794 - top-5-accuracy: 1.0000 - val_loss: 0.3410 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
            "Epoch 37/60\n",
            "31/31 [==============================] - 7s 223ms/step - loss: 0.0853 - accuracy: 0.9702 - top-5-accuracy: 1.0000 - val_loss: 0.4112 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9876\n",
            "Epoch 38/60\n",
            "31/31 [==============================] - 7s 237ms/step - loss: 0.1076 - accuracy: 0.9650 - top-5-accuracy: 1.0000 - val_loss: 0.3859 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
            "Epoch 39/60\n",
            "31/31 [==============================] - 7s 220ms/step - loss: 0.1066 - accuracy: 0.9619 - top-5-accuracy: 1.0000 - val_loss: 0.3841 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
            "Epoch 40/60\n",
            "31/31 [==============================] - 7s 224ms/step - loss: 0.0445 - accuracy: 0.9887 - top-5-accuracy: 1.0000 - val_loss: 0.3772 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
            "Epoch 41/60\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0215 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.4037 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9938\n",
            "Epoch 42/60\n",
            "31/31 [==============================] - 6s 195ms/step - loss: 0.0329 - accuracy: 0.9907 - top-5-accuracy: 1.0000 - val_loss: 0.3856 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9938\n",
            "Epoch 43/60\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 0.0263 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.3997 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9938\n",
            "Epoch 44/60\n",
            "31/31 [==============================] - 7s 224ms/step - loss: 0.0087 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3419 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
            "Epoch 45/60\n",
            "31/31 [==============================] - 7s 226ms/step - loss: 0.0113 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.3244 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9876\n",
            "Epoch 46/60\n",
            "31/31 [==============================] - 7s 215ms/step - loss: 0.0321 - accuracy: 0.9959 - top-5-accuracy: 1.0000 - val_loss: 0.3821 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9938\n",
            "Epoch 47/60\n",
            "31/31 [==============================] - 6s 205ms/step - loss: 0.0227 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.4624 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9938\n",
            "Epoch 48/60\n",
            "31/31 [==============================] - 6s 190ms/step - loss: 0.0126 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4342 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
            "Epoch 49/60\n",
            "31/31 [==============================] - 6s 203ms/step - loss: 0.0106 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.4281 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
            "Epoch 50/60\n",
            "31/31 [==============================] - 6s 206ms/step - loss: 0.0058 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3549 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
            "Epoch 51/60\n",
            "31/31 [==============================] - 6s 192ms/step - loss: 0.0036 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3800 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
            "Epoch 52/60\n",
            "31/31 [==============================] - 6s 196ms/step - loss: 0.0046 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3865 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
            "Epoch 53/60\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0034 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3930 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9938\n",
            "Epoch 54/60\n",
            "31/31 [==============================] - 6s 202ms/step - loss: 0.0018 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4097 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
            "Epoch 55/60\n",
            "31/31 [==============================] - 6s 200ms/step - loss: 0.0016 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4117 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
            "Epoch 56/60\n",
            "31/31 [==============================] - 6s 192ms/step - loss: 0.0018 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4087 - val_accuracy: 0.9068 - val_top-5-accuracy: 1.0000\n",
            "Epoch 57/60\n",
            "31/31 [==============================] - 7s 214ms/step - loss: 0.0015 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4107 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
            "Epoch 58/60\n",
            "31/31 [==============================] - 6s 204ms/step - loss: 0.0014 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4187 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
            "Epoch 59/60\n",
            "31/31 [==============================] - 7s 221ms/step - loss: 0.0014 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4170 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
            "Epoch 60/60\n",
            "31/31 [==============================] - 7s 221ms/step - loss: 0.0013 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4128 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
            "20/20 [==============================] - 2s 86ms/step - loss: 1.1355 - accuracy: 0.7852 - top-5-accuracy: 0.9656\n",
            "Test accuracy: 78.52%\n",
            "Test top 5 accuracy: 96.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "rDdVMlzf0H4u"
      }
    }
  ]
}